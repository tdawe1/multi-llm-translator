# Multi-LLM Translation Assistant

An automated Python translation assistant that monitors for jobs, processes documents (.docx, .pptx, .xlsx), and uses multiple LLMs (GPT, Claude, Gemini) for high-quality, comparable translations and critiques.

---

## Overview

This tool is designed to automate the "first pass" of a translation workflow. It solves the tedious problem of manually preparing documents, sending them to various services, and collating the results. It can watch for new jobs in a CSV file or new files in a "hot folder," process them automatically, and regenerate fully-formatted documents with the translated text.

It features several powerful operation modes, from simple parallel translation to an advanced "critique" mode where LLMs review and refine each other's work.

## Integration with GengoWatcher

This assistant is designed to be the perfect companion to the [**GengoWatcher-Public**](https://github.com/tdawe1/GengoWatcher-Public) application.

While `GengoWatcher-Public` excels at finding and notifying you of new translation jobs, this Multi-LLM Translation Assistant takes the next step: **it automatically processes the jobs that GengoWatcher finds.** By pointing this assistant to the CSV file generated by GengoWatcher, you can create a fully automated pipeline: from job discovery to final, multi-model translation.

## Key Features

- **Automated Job Monitoring**: Watches a CSV file for new job tickets or a local "hot folder" for new file drops.
- **Multi-LLM Engine**: Utilizes GPT, Claude, and Gemini to perform translations, allowing for robust quality comparison.
- **Complex Document Support**: Extracts text from and regenerates `.docx`, `.pptx`, and `.xlsx` files, preserving basic structure.
- **Advanced Workflows**:
    - **Parallel Mode**: Get translations from all LLMs simultaneously.
    - **Critique Mode**: One LLM translates, and the others provide constructive feedback and a refined version.
- **Configurable & Professional**:
    - **Glossary Support**: Enforces consistent terminology via a `glossary.json` file.
    - **External Prompts**: Easily customize prompts in the `templates/` directory without changing code.
    - **Colored Logging**: Clean, color-coded console output and detailed `app.log` file for easy debugging.

## How It Works

1.  **Monitor**: The application runs in two parallel threads:
    - A **CSV Worker** watches `monitor/jobs_feed.csv` for new entries.
    - A **Hot Folder Worker** watches the `uploads/` directory for any new files.
2.  **Fetch**: When a job is triggered, the tool extracts all text from the source document (`.txt`, `.docx`, etc.).
3.  **Process**: Based on the `OPERATION_MODE` set in your `.env` file, it either translates the text in parallel or runs the advanced critique workflow.
4.  **Regenerate**: The tool creates new, translated documents in the `outputs/` directory, preserving the original file format.

## Getting Started

### Prerequisites

- Python 3.9+
- Your `jobs_feed.csv` file, ideally generated by [GengoWatcher-Public](https://github.com/tdawe1/GengoWatcher-Public).

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/YOUR_USERNAME/multi-llm-translator.git
    cd multi-llm-translator
    ```

2.  **Set up a Python virtual environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate
    ```

3.  **Install the required dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure your API keys:**
    - Rename the example environment file:
      ```bash
      mv .env.example .env
      ```
    - Edit the `.env` file and add your API keys from OpenAI, Anthropic, and Google.

## Usage

Once configured, you can start the application by running:

```bash
python app.py
```

The application will start, and you will see colored log output in your terminal. You can now trigger jobs by either having `GengoWatcher-Public` add entries to `monitor/jobs_feed.csv` or by dropping files into the `uploads/` directory.

### Running Test Files

A utility script is included to quickly populate your `uploads` folder with test documents.

```bash
# First, make it executable (only needs to be done once)
chmod +x create_test_files.sh

# Run it to generate a fresh set of test files
./create_test_files.sh
```

## Configuration

All major settings are controlled from your `.env` file.

| Setting                   | Options                           | Description                                                                                              |
| ------------------------- | --------------------------------- | -------------------------------------------------------------------------------------------------------- |
| `OPERATION_MODE`          | `SIMPLE`, `PARALLEL`, `CRITIQUE`  | **SIMPLE**: Translates with `PRIMARY_MODEL` only. **PARALLEL**: Translates with all LLMs. **CRITIQUE**: One translates, others review. |
| `PRIMARY_MODEL`           | `gpt`, `claude`, `gemini`         | The main model to use for `SIMPLE` and `CRITIQUE` modes.                                                 |
| `GLOSSARY_PATH`           | `glossary.json` (or other path)   | Path to a JSON file containing key-value pairs for terminology enforcement.                                |
| `DEFAULT_SOURCE_LANGUAGE` | `Japanese`, `English`, etc.       | The source language to assume for hot folder files when not specified in the filename.                     |
| `DEFAULT_TARGET_LANGUAGE` | `English`, `German`, etc.         | The target language to assume for hot folder files.                                                      |
| `DUMMY_MODE`              | `True`, `False`                   | If `True`, the app simulates API calls without using tokens, allowing for free testing of the logic.     |

## Cost Estimation

The cost of using this application is directly tied to the API usage of the underlying LLMs. The total cost depends on four main factors: **document size**, **operation mode**, **model choice**, and the **provider's pricing**.

### Cost Factors

1.  **Document Size**: Larger documents mean more tokens, which directly increases cost.
2.  **Operation Mode (`.env`)**: This is the biggest driver of cost.
    - **`SIMPLE`**: The cheapest mode (1 API call).
    - **`PARALLEL`**: Moderately expensive (3 API calls).
    - **`CRITIQUE`**: The most expensive mode (1 translation + 2 larger critique calls). Can be **5-7x more expensive** than `SIMPLE` mode.
3.  **Model Choice**: The models used in this script were chosen for their balance of quality and cost. However, prices can change.
4.  **API Pricing**: You should always refer to the official pricing pages for the most current information.
    - [OpenAI Pricing](https://openai.com/pricing)
    - [Anthropic (Claude) Pricing](https://www.anthropic.com/pricing)
    - [Google (Gemini) Pricing](https://ai.google.dev/pricing)

### Example Cost Breakdown

Here is a rough cost estimate for processing a **1,000-word document** (approx. 1,300 tokens), based on pricing as of July 2025.

| Mode                      | Calculation Breakdown                                                                | Estimated Cost                     |
| :------------------------ | :----------------------------------------------------------------------------------- | :--------------------------------- |
| **`SIMPLE`** (GPT Primary) | Cost of one translation with GPT-4o-mini.                                            | **~ $0.001** (a tenth of a cent)   |
| **`PARALLEL`**            | Cost of (1 GPT + 1 Claude + 1 Gemini) translations.                                  | **~ $0.04** (about 4 cents)      |
| **`CRITIQUE`** (GPT Primary)  | Cost of (1 GPT translation + 1 Claude critique + 1 Gemini critique). The critique calls have much larger inputs. | **~ $0.06** (about 6 cents)      |

As you can see, costs can rise quickly with the more advanced modes.

### Cost Management Best Practices

- **Use `DUMMY_MODE=True` for all testing.**
- Use `SIMPLE` mode for bulk, day-to-day translations.
- Use `PARALLEL` or `CRITIQUE` modes for important jobs that require quality comparison or refinement.
- **Set billing alerts** in your OpenAI, Anthropic, and Google Cloud dashboards to prevent unexpected costs.

## Future Work

- [ ] **Streamlit UI**: Build the user interface for reviewing translations side-by-side.
- [ ] **Advanced Regeneration**: Add support for preserving more complex formatting (styles, tables, images).
- [ ] **Job Status Tracking**: Implement a more robust system for tracking job status (`pending`, `processed`, `error`).